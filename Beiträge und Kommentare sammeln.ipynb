{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Skript zum Digitalreport 2018\n",
    "Das folgende Skript kann zum Sammeln von Daten von Facebook Pages genutzt werden.\n",
    "Weitere Infos: https://digitalreport.at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import facepy\n",
    "from facepy import GraphAPI\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facbook API Access token einfügen\n",
    "# Etwa über https://developers.facebook.com/tools/explorer/\n",
    "access_token = 'TOKEN_HIER_EINFÜGEN'\n",
    "\n",
    "# Usernames (aus der URL) der Seiten kopieren, deren Daten gesammelt werden sollen\n",
    "polpages = ['christian.kern.spoe', 'matthias.strolz','hcstrache', \n",
    "            'sebastiankurz.at','peterpilz',\n",
    "            'diegruenen', 'NeosDasNeueOesterreich', 'Sozialdemokratie',\n",
    "            'Volkspartei', 'listepilz.at', 'fpoe', 'wernerkogler']\n",
    "\n",
    "graph = GraphAPI(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Sammeln der Beiträge und Reactions\n",
    "def get_posts_as_df(fbpage):\n",
    "    since_date = '2000-01-01' #'2016-08-01'\n",
    "    until_date = '2018-06-01'\n",
    "    fbpageid = graph.get(fbpage)['id']\n",
    "    pages = graph.get('v3.0/'+fbpageid+'/posts?fields=message,message_tags,link,created_time,type,name,id,comments.filter(stream).summary(true).limit(0),shares,from,source,status_type,updated_time,reactions.type(LIKE).limit(0).summary(1).as(like),reactions.type(LOVE).limit(0).summary(1).as(love),reactions.type(HAHA).limit(0).summary(1).as(haha),reactions.type(WOW).limit(0).summary(1).as(wow),reactions.type(SAD).limit(0).summary(1).as(sad),reactions.type(ANGRY).limit(0).summary(1).as(angry)&since='+since_date+'&until='+until_date+'&limit=100', page=True)\n",
    "    posts = []\n",
    "    for page in pages:\n",
    "        for post in page['data']:\n",
    "            #del(post['comments'])\n",
    "            post['comments'] = post['comments']['summary']['total_count']\n",
    "            post['like'] = post['like']['summary']['total_count']\n",
    "            post['love'] = post['love']['summary']['total_count']\n",
    "            post['haha'] = post['haha']['summary']['total_count']\n",
    "            post['wow'] = post['wow']['summary']['total_count']\n",
    "            post['sad'] = post['sad']['summary']['total_count']\n",
    "            post['angry'] = post['angry']['summary']['total_count']\n",
    "            post['from'] = post['from']['name']\n",
    "            #post['reactions'] = post['reactions']['summary']['total_count']\n",
    "            try:\n",
    "                post['shares'] = post['shares']['count']\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                post['hostname'] = urlparse(post['link']).hostname\n",
    "            except:\n",
    "                pass\n",
    "            posts.append(post)\n",
    "    df = pd.DataFrame(posts)\n",
    "    #df['hostname'] = df['link'].apply(lambda x: urlparse(x).hostname)\n",
    "    df['created_time'] = pd.to_datetime(df['created_time'])\n",
    "    df.set_index('created_time', inplace=True)\n",
    "    df['post'] = 1\n",
    "    print(fbpage + ': ' +str(len(posts)))\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sammelt die Beiträge für alle angegebenen Seiten\n",
    "posts = get_posts_as_df(polpages[0])\n",
    "for seite in polpages[1:]:\n",
    "    posts = posts.append(get_posts_as_df(seite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hostnames aufräumen\n",
    "import re\n",
    "def hostname_cleanup(hostname):\n",
    "    replacements = [\n",
    "        ('.*facebook\\.com',''),\n",
    "        ('^www\\.',''),\n",
    "        ('^ww1\\.',''),\n",
    "        ('^mobil\\.','',),\n",
    "        ('^m\\.','',),\n",
    "        ('.*orf\\.at$','orf.at',),\n",
    "        ('.*tt.com$','tt.com',),\n",
    "        ('^cms.','',),\n",
    "        ('.*\\.spoe\\.at$','spoe.at',),\n",
    "        ('.*\\.gruene\\.at$.','gruene.at',),\n",
    "        ('.*\\.neos\\.eu$','neos.eu',),\n",
    "        ('.*\\.sebastian-kurz\\.at$','sebastian-kurz.at',),\n",
    "        ('.*\\.oe24\\.at$','oe24.at',),\n",
    "        ('.*instagram.com$','',),\n",
    "        ('.*bit.ly$','',),\n",
    "        ('.*kurz-link.at$','',),\n",
    "        ('.*t.co$','',),\n",
    "        ('.*yt2fb.com*$','',),\n",
    "        ('.*youtube.com$','',),\n",
    "        ('.*youtu.be$','',),\n",
    "        ('.*yumpu.com$','',),\n",
    "        ('.*giphy.com*$','',),\n",
    "        ('.*gph.is*$','',),\n",
    "        ('.*goo.gl*$','',),\n",
    "        ('.*twibbon.com$','',),\n",
    "        ('.*buff.ly$','',),\n",
    "        ('.*tinyurl.com$','',),\n",
    "        ('.*google.at$','',),\n",
    "        ('.*lsh.re$','',),\n",
    "        ('.*google.de$','',),\n",
    "        ('.*snip.ly$','',),\n",
    "        ('.*google.com$','',),\n",
    "    ]\n",
    "    \n",
    "    if pd.notnull(hostname):\n",
    "        for old, new in replacements:\n",
    "            hostname = re.sub(old,new,hostname)\n",
    "            #print(row['hostname'])\n",
    "    return(hostname)\n",
    "posts['hostname_clean'] = posts['hostname'].apply(hostname_cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reactions anzeigen\n",
    "posts.groupby('from').aggregate(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links anzeigen\n",
    "posts.groupby(['from', 'hostname_clean'])['post'].aggregate(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Als .xlsx für Excel speichern\n",
    "posts.to_excel('digitalreport.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud erstellen\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stop_words import get_stop_words\n",
    "import random\n",
    "from colormath.color_objects import CMYKColor, HSLColor\n",
    "from colormath.color_conversions import convert_color\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def wordy(page):\n",
    "    d = ''\n",
    "\n",
    "    # Read the whole text.\n",
    "    text = \"\"\n",
    "    for i, c in posts.where(posts['from'] == page)['message'].dropna().iteritems():\n",
    "        text = text + \" \" + c\n",
    "        \n",
    "    def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "        #abstufung = random.randint(20,100)/100\n",
    "        abstufung = random.choice([0.2,0.4,0.6,0.8,1])\n",
    "        c = 90 * abstufung \n",
    "        m = 100 * abstufung\n",
    "        cmyk = CMYKColor(c,m,0,0)\n",
    "        hsl = convert_color(cmyk, HSLColor)\n",
    "        return \"hsl({}, 100%, {}%)\".format(int(hsl.hsl_h), int(hsl.hsl_l+100))\n",
    "        #return \"hsl(%d, 100%%, 55%%)\" % random.randint(193, 246)\n",
    "        \n",
    "\n",
    "    # read the mask image\n",
    "    image_mask = np.array(Image.open(\"cloud.png\")) \n",
    "\n",
    "    stopwords = get_stop_words('de')\n",
    "    stopwords.append(\"created_time\")\n",
    "    stopwords.append(\"NaN\")\n",
    "    stopwords.append(\"RT\")\n",
    "    stopwords.append(\"https\")\n",
    "    stopwords.append(\"co\")\n",
    "    stopwords.append(\"http\")\n",
    "    stopwords.append(\"goo\")\n",
    "    stopwords.append(\"gl\")\n",
    "    stopwords.append(\"ly\")\n",
    "    stopwords.append(\"bit\")\n",
    "    stopwords.append(\"be\")\n",
    "    stopwords.append(\"www\")\n",
    "    stopwords.append(\"artikel\")\n",
    "    stopwords.append(\"fpoe\")\n",
    "    stopwords.append(\"at\")\n",
    "    stopwords.append(\"gibt\")\n",
    "    stopwords.append(\"geht\")\n",
    "    stopwords.append(\"wurde\")\n",
    "    stopwords.append(\"to\")\n",
    "    stopwords.append(\"the\")\n",
    "    stopwords.append(\"and\")\n",
    "    stopwords.append(\"of\")\n",
    "    stopwords.append(\"daher\")\n",
    "    stopwords.append(\"Mehr\")\n",
    "    stopwords.append(\"goo\")\n",
    "    stopwords.append(\"beim\")\n",
    "    stopwords.append(\"link\")\n",
    "    stopwords.append(\"mehr\")\n",
    "    stopwords.append(\"Video\")\n",
    "    stopwords.append(\"Unsere\")\n",
    "    stopwords.append(\"heute\")\n",
    "    stopwords.append(\"daher\")\n",
    "    stopwords.append(\"Infos\")\n",
    "    stopwords.append(\"is\")\n",
    "    stopwords.append(\"by\")\n",
    "    stopwords.append(\"for\")\n",
    "    stopwords.append(\"it\")\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000, stopwords=stopwords, width=1200, height=584, collocations=False, scale=1,\n",
    "                   min_font_size=4, max_font_size=200) #mask=alice_mask,prefer_horizontal=1 regexp=r\"\\w[\\w'/-]+\"\n",
    "    \n",
    "    # generate text file to use with other tools\n",
    "    wc.generate(text)\n",
    "    with open('freq'+page+'.txt', 'w', encoding='UTF-8') as f:\n",
    "        for freq, word in sorted(((value,key) for (key,value) in wc.process_text(text).items()), reverse=True):\n",
    "            f.write(str(word) + ' ' + str(freq*4) + '\\r\\n')\n",
    "\n",
    "    \n",
    "\n",
    "    # show\n",
    "    plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3), interpolation='bilinear')\n",
    "    \n",
    "    # store to file\n",
    "    wc.to_file(\"wordcloud_\"+page+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\"LISTE PILZ\",\"Peter Pilz\",\"Die Grünen\",\"NEOS\",\"Matthias Strolz\",\"SPÖ\",\"Christian Kern\",\"Volkspartei\",\"Sebastian Kurz\",\"FPÖ\",\"HC Strache\"]\n",
    "for p in pages:\n",
    "    wordy(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Kommentare sammeln\n",
    "def get_comments(postid, post_author):\n",
    "    pages = graph.get(postid+'/comments?fields=attachment,message,message_tags,created_time&limit=100&filter=stream', page=True)\n",
    "    comments = []\n",
    "    for page in pages:\n",
    "        for comment in page['data']:\n",
    "            comment['post_id'] = postid\n",
    "            comment['post_author'] = post_author\n",
    "            if 'attachment' in comment:\n",
    "                comment['attachment_type'] = comment['attachment']['type']\n",
    "                comment['attachment'] = comment['attachment']['url']\n",
    "            comments.append(comment)\n",
    "    df = df = pd.DataFrame(comments)\n",
    "    df['created_time'] = pd.to_datetime(df['created_time'])\n",
    "    df.set_index('created_time', inplace=True)\n",
    "    df['comment'] = 1\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kommentare sammeln\n",
    "all_comments = get_comments(allposts.iloc[-1]['id'],allposts.iloc[-1]['from'])\n",
    "for index, post in allposts[1:-1].iterrows():\n",
    "    #print (post['id'])\n",
    "    try:\n",
    "        all_comments = all_comments.append(get_comments(post['id'],post['from']))\n",
    "    except:\n",
    "        pass\n",
    "all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf pages - websites zur Nutzung mit Gephi\n",
    "with open('page-websites.gdf', 'w') as f:\n",
    "    f.write('nodedef>name VARCHAR,label VARCHAR\\n')\n",
    "    f.write('edgedef>node1 VARCHAR,node2 VARCHAR,directed BOOLEAN\\n')\n",
    "    for i, row in allposts.iterrows():\n",
    "        try:\n",
    "            f.write(row['from'] + ',' + row['hostname'] + ',TRUE\\n')\n",
    "        except:\n",
    "            #print(row['hostname'])\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf pages - shares zur Nutzung mit Gephi \n",
    "with open('page-shares.gdf', 'w') as f:\n",
    "    usernames = {}\n",
    "    f.write('nodedef>name VARCHAR,label VARCHAR\\n')\n",
    "    f.write('edgedef>node1 VARCHAR,node2 VARCHAR,directed BOOLEAN\\n')\n",
    "    for i, row in allposts[\"2018-1-1\":\"2018-6-1\"].iterrows():\n",
    "        try:\n",
    "            if row['hostname'] == 'www.facebook.com':\n",
    "                username = urlparse(row['link']).path.split('/')[1])['name']\n",
    "                if username in usernames:\n",
    "                    f.write(row['from'] + ',' + usernames[username] + ',TRUE\\n')\n",
    "                else:\n",
    "                    usernames[username] = graph.get(username)['name']\n",
    "                    f.write(row['from'] + ',' + usernames[username] + ',TRUE\\n')\n",
    "        except:\n",
    "            print(row['hostname'])\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf pages - mentions zur Nutzung mit Gephi\n",
    "with open('page-mentions.gdf', 'w') as f:\n",
    "    f.write('nodedef>name VARCHAR,label VARCHAR\\n')\n",
    "    f.write('edgedef>node1 VARCHAR,node2 VARCHAR,directed BOOLEAN\\n')\n",
    "    for i, row in allposts[\"2017-1-1\":\"2018-6-1\"].iterrows():\n",
    "        try:\n",
    "            for mention in row['message_tags']:\n",
    "                #print(mention['name'])\n",
    "                f.write(row['from'] + ',' + mention['name'] + ',TRUE\\n')\n",
    "        except:\n",
    "            #print(row['hostname'])\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
